{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 横向堆叠（索引完全相同时）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 09:01:05,040 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'\n",
      "2020-03-26 09:01:05,042 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,051 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'lower_case_table_names'\n",
      "2020-03-26 09:01:05,052 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,060 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()\n",
      "2020-03-26 09:01:05,060 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,067 INFO sqlalchemy.engine.base.Engine show collation where `Charset` = 'utf8mb4' and `Collation` = 'utf8mb4_bin'\n",
      "2020-03-26 09:01:05,068 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,080 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1\n",
      "2020-03-26 09:01:05,082 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,087 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1\n",
      "2020-03-26 09:01:05,088 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,095 INFO sqlalchemy.engine.base.Engine SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8mb4) COLLATE utf8mb4_bin AS anon_1\n",
      "2020-03-26 09:01:05,096 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,102 INFO sqlalchemy.engine.base.Engine DESCRIBE `meal_order_detail1`\n",
      "2020-03-26 09:01:05,102 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,113 INFO sqlalchemy.engine.base.Engine SHOW FULL TABLES FROM `testdb`\n",
      "2020-03-26 09:01:05,115 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,125 INFO sqlalchemy.engine.base.Engine SHOW CREATE TABLE `meal_order_detail1`\n",
      "2020-03-26 09:01:05,126 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:01:05,141 INFO sqlalchemy.engine.base.Engine SELECT meal_order_detail1.detail_id, meal_order_detail1.order_id, meal_order_detail1.dishes_id, meal_order_detail1.logicprn_name, meal_order_detail1.parent_class_name, meal_order_detail1.dishes_name, meal_order_detail1.itemis_add, meal_order_detail1.counts, meal_order_detail1.amounts, meal_order_detail1.cost, meal_order_detail1.place_order_time, meal_order_detail1.discount_amt, meal_order_detail1.discount_reason, meal_order_detail1.kick_back, meal_order_detail1.add_inprice, meal_order_detail1.add_info, meal_order_detail1.bar_code, meal_order_detail1.picture_file, meal_order_detail1.emp_id \n",
      "FROM meal_order_detail1\n",
      "2020-03-26 09:01:05,142 INFO sqlalchemy.engine.base.Engine {}\n",
      "合并后df1和df2的大小分别为：(2779, 19)\n",
      "数据库大小：(2779, 19)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "conn=create_engine('mysql+pymysql://root:Cuiqin:233@localhost:3306/testdb?charset=utf8',encoding='utf-8',echo=True)\n",
    "detail1=pd.read_sql('meal_order_detail1',conn)\n",
    "df1=detail.iloc[:,:10]\n",
    "df2=detail.iloc[:,10:]\n",
    "print('合并后df1和df2的大小分别为：'+str(pd.concat([df1,df2],axis=1,join='inner').shape))\n",
    "print('数据库大小：'+str(pd.concat([df1,df2],axis=1,join='outer').shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df3 (2779, 38)\n",
      "(0, 38)\n"
     ]
    }
   ],
   "source": [
    "df3 = detail.iloc[:1500,:]\n",
    "df4 = detail.iloc[1500:,:]\n",
    "print(\"df3\",pd.concat([df3,df4],axis=1,join='outer').shape)\n",
    "print(pd.concat([df3,df4],axis=1,join='inner').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append方法:(1500, 19, 1279, 19)\n",
      "总想堆叠后的数据框：(2779, 19)\n"
     ]
    }
   ],
   "source": [
    "print('append方法:'+str(df3.shape+df4.shape))\n",
    "print('总想堆叠后的数据框：'+str(df3.append(df4).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge合并\n",
    "order = pd.read_csv('./data2/meal_order_info.csv',sep=',',encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detail订单那详情表的原始形状为： (2779, 19)\n",
      "order订单星系的原始形状 (945, 21)\n",
      "合并以后的形状 (2779, 40)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "order = pd.read_csv('./data2/meal_order_info.csv',sep=',',encoding='gb18030')\n",
    "order['info_id'] = order['info_id'].astype('str')\n",
    "order_detail1 = pd.merge(detail1,order,left_on='order_id',right_on='info_id')\n",
    "print('detail订单那详情表的原始形状为：',detail1.shape)\n",
    "print('order订单星系的原始形状',order.shape)\n",
    "print('合并以后的形状',order_detail1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info_id                 int64\n",
      "emp_id                  int64\n",
      "number_consumers        int64\n",
      "mode                  float64\n",
      "dining_table_id         int64\n",
      "dining_table_name       int64\n",
      "expenditure             int64\n",
      "dishes_count            int64\n",
      "accounts_payable        int64\n",
      "use_start_time         object\n",
      "check_closed          float64\n",
      "lock_time              object\n",
      "cashier_id            float64\n",
      "pc_id                 float64\n",
      "order_number          float64\n",
      "org_id                  int64\n",
      "print_doc_bill_num    float64\n",
      "lock_table_info       float64\n",
      "order_status            int64\n",
      "phone                   int64\n",
      "name                   object\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-aca13045a6ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0morder_detail1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'order_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder_detail1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'order_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0morder_detail1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdetail1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'order_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder_detail1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mS:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   7244\u001b[0m         \u001b[1;31m# For SparseDataFrame's benefit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7245\u001b[0m         return self._join_compat(\n\u001b[1;32m-> 7246\u001b[1;33m             \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7247\u001b[0m         )\n\u001b[0;32m   7248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mS:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   7267\u001b[0m                 \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7268\u001b[0m                 \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7269\u001b[1;33m                 \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7270\u001b[0m             )\n\u001b[0;32m   7271\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mS:\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     )\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mS:\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# to avoid incompat dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;31m# If argument passed to validate,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mS:\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m                     \u001b[0minferred_right\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minferred_left\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m                 ):\n\u001b[1;32m-> 1138\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[1;31m# datetimelikes must match exactly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "order.rename({'info_id':'order_id'},inplace=True)\n",
    "print(order.dtypes)\n",
    "order_detail1['order_id']=order_detail1['order_id'].apply(int)\n",
    "order_detail1=detail1.join(order,on='order_id',rsuffix='1')\n",
    "print(order_detail1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID System cpu\n",
      "0   1  win10  i7\n",
      "1   2  win10  i5\n",
      "2   3   win7  i3\n",
      "3   4  win10  i7\n",
      "4   5   win8  i7\n",
      "5   6   win7  i5\n",
      "6   7   win7  i5\n",
      "7   8   win7  i5\n",
      "8   9   win8  i3\n"
     ]
    }
   ],
   "source": [
    "# 重叠合并\n",
    "dict1 = {'ID':[1,2,3,4,5,6,7,8,9],\n",
    "        'System':['win10','win10',np.nan,'win10',np.nan,np.nan,'win7','win7','win8'],\n",
    "        'cpu':['i7','i5',np.nan,'i7',np.nan,np.nan,'i5','i5','i3']}\n",
    "dict2 = {'ID':[1,2,3,4,5,6,7,8,9],\n",
    "        'System':[np.nan,np.nan,'win7',np.nan,'win8','win7',np.nan,np.nan,np.nan],\n",
    "        'cpu':[np.nan,np.nan,'i3',np.nan,'i7','i5',np.nan,np.nan,np.nan]}\n",
    "df5 = pd.DataFrame(dict1)\n",
    "df6 = pd.DataFrame(dict2)\n",
    "print(df5.combine_first(df6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "conn=create_engine('mysql+pymysql://root:Cuiqin:233@localhost:3306/testdb?charset=utf8',encoding='utf-8',echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 09:19:54,085 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'\n",
      "2020-03-26 09:19:54,085 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,093 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'lower_case_table_names'\n",
      "2020-03-26 09:19:54,093 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,104 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()\n",
      "2020-03-26 09:19:54,105 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,110 INFO sqlalchemy.engine.base.Engine show collation where `Charset` = 'utf8mb4' and `Collation` = 'utf8mb4_bin'\n",
      "2020-03-26 09:19:54,113 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,122 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1\n",
      "2020-03-26 09:19:54,122 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,126 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1\n",
      "2020-03-26 09:19:54,129 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,133 INFO sqlalchemy.engine.base.Engine SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8mb4) COLLATE utf8mb4_bin AS anon_1\n",
      "2020-03-26 09:19:54,136 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,141 INFO sqlalchemy.engine.base.Engine DESCRIBE `meal_order_detail1`\n",
      "2020-03-26 09:19:54,142 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,151 INFO sqlalchemy.engine.base.Engine SHOW FULL TABLES FROM `testdb`\n",
      "2020-03-26 09:19:54,153 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,160 INFO sqlalchemy.engine.base.Engine SHOW CREATE TABLE `meal_order_detail1`\n",
      "2020-03-26 09:19:54,160 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,176 INFO sqlalchemy.engine.base.Engine SELECT meal_order_detail1.detail_id, meal_order_detail1.order_id, meal_order_detail1.dishes_id, meal_order_detail1.logicprn_name, meal_order_detail1.parent_class_name, meal_order_detail1.dishes_name, meal_order_detail1.itemis_add, meal_order_detail1.counts, meal_order_detail1.amounts, meal_order_detail1.cost, meal_order_detail1.place_order_time, meal_order_detail1.discount_amt, meal_order_detail1.discount_reason, meal_order_detail1.kick_back, meal_order_detail1.add_inprice, meal_order_detail1.add_info, meal_order_detail1.bar_code, meal_order_detail1.picture_file, meal_order_detail1.emp_id \n",
      "FROM meal_order_detail1\n",
      "2020-03-26 09:19:54,179 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,474 INFO sqlalchemy.engine.base.Engine DESCRIBE `meal_order_detail2`\n",
      "2020-03-26 09:19:54,478 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,488 INFO sqlalchemy.engine.base.Engine SHOW FULL TABLES FROM `testdb`\n",
      "2020-03-26 09:19:54,489 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,493 INFO sqlalchemy.engine.base.Engine SHOW CREATE TABLE `meal_order_detail2`\n",
      "2020-03-26 09:19:54,495 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,507 INFO sqlalchemy.engine.base.Engine SELECT meal_order_detail2.detail_id, meal_order_detail2.order_id, meal_order_detail2.dishes_id, meal_order_detail2.logicprn_name, meal_order_detail2.parent_class_name, meal_order_detail2.dishes_name, meal_order_detail2.itemis_add, meal_order_detail2.counts, meal_order_detail2.amounts, meal_order_detail2.cost, meal_order_detail2.place_order_time, meal_order_detail2.discount_amt, meal_order_detail2.discount_reason, meal_order_detail2.kick_back, meal_order_detail2.add_inprice, meal_order_detail2.add_info, meal_order_detail2.bar_code, meal_order_detail2.picture_file, meal_order_detail2.emp_id \n",
      "FROM meal_order_detail2\n",
      "2020-03-26 09:19:54,507 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,971 INFO sqlalchemy.engine.base.Engine DESCRIBE `meal_order_detail3`\n",
      "2020-03-26 09:19:54,975 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,983 INFO sqlalchemy.engine.base.Engine SHOW FULL TABLES FROM `testdb`\n",
      "2020-03-26 09:19:54,984 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:54,989 INFO sqlalchemy.engine.base.Engine SHOW CREATE TABLE `meal_order_detail3`\n",
      "2020-03-26 09:19:54,991 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-03-26 09:19:55,004 INFO sqlalchemy.engine.base.Engine SELECT meal_order_detail3.detail_id, meal_order_detail3.order_id, meal_order_detail3.dishes_id, meal_order_detail3.logicprn_name, meal_order_detail3.parent_class_name, meal_order_detail3.dishes_name, meal_order_detail3.itemis_add, meal_order_detail3.counts, meal_order_detail3.amounts, meal_order_detail3.cost, meal_order_detail3.place_order_time, meal_order_detail3.discount_amt, meal_order_detail3.discount_reason, meal_order_detail3.kick_back, meal_order_detail3.add_inprice, meal_order_detail3.add_info, meal_order_detail3.bar_code, meal_order_detail3.picture_file, meal_order_detail3.emp_id \n",
      "FROM meal_order_detail3\n",
      "2020-03-26 09:19:55,004 INFO sqlalchemy.engine.base.Engine {}\n",
      "(10037, 19)\n"
     ]
    }
   ],
   "source": [
    "detail1 = pd.read_sql('meal_order_detail1',conn)\n",
    "detail2 = pd.read_sql('meal_order_detail2',conn)\n",
    "detail3 = pd.read_sql('meal_order_detail3',conn)\n",
    "detail = detail1.append(detail2)\n",
    "detail = detail.append(detail3)\n",
    "print(detail.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14713, 76)\n"
     ]
    }
   ],
   "source": [
    "order=pd.read_csv('./data2/meal_order_info.csv',sep=',',encoding='gb18030',engine='python')\n",
    "user=pd.read_excel('./data2/users_info.xlsx')\n",
    "order['emp_id'] = order['emp_id'].astype('str')\n",
    "user['USER_ID'] = user['USER_ID'].astype('str')\n",
    "order['info_id'] = order['info_id'].astype('str')\n",
    "data = pd.merge(detail,order,left_on=['order_id','emp_id'],right_on=['info_id','emp_id'])\n",
    "data = pd.merge(data,user,left_on='emp_id',right_on='USER_ID',how='inner')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10037\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "detail1 = pd.read_csv('./data2/detail.csv',index_col=0,encoding='gbk')\n",
    "\n",
    "# 定义去重函数方法\n",
    "def delRep(list1):\n",
    "    list2=[]\n",
    "    for i in list1:\n",
    "        if i not in list2:\n",
    "            list2.append(i)\n",
    "    return list2\n",
    "dishes=list(detail1['dishes_name'])\n",
    "print(len(dishes))\n",
    "dish=delRep(dishes)\n",
    "print(len(dish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10037\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "# 方法二\n",
    "print(len(dishes))\n",
    "dish_set=set(dishes)\n",
    "print(len(dish_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_duplicates方法去重之后菜品总数为 145\n"
     ]
    }
   ],
   "source": [
    "# dishes_name\n",
    "dishes_name = detail1['dishes_name'].drop_duplicates()\n",
    "print('drop_duplicates方法去重之后菜品总数为',len(dishes_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重之前订单详情表的形状为： (10037, 18)\n",
      "(942, 18)\n"
     ]
    }
   ],
   "source": [
    "print('去重之前订单详情表的形状为：',detail1.shape)\n",
    "shapeDet = detail1.drop_duplicates(subset=['order_id',\n",
    "                                         'emp_id']).shape\n",
    "print(shapeDet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           counts   amounts\n",
      "counts   1.000000 -0.229968\n",
      "amounts -0.229968  1.000000\n"
     ]
    }
   ],
   "source": [
    "corrDet = detail1[['counts','amounts']].corr(method='kendall')\n",
    "print(corrDet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           counts   amounts\n",
      "counts   1.000000 -0.159264\n",
      "amounts -0.159264  1.000000\n"
     ]
    }
   ],
   "source": [
    "corrDet1=detail1[['dishes_name','counts','amounts']].corr(method='pearson')\n",
    "print(corrDet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   order_id  dishes_id  logicprn_name  parent_class_name  \\\n",
      "order_id               True      False          False              False   \n",
      "dishes_id             False       True          False              False   \n",
      "logicprn_name         False      False           True               True   \n",
      "parent_class_name     False      False           True               True   \n",
      "dishes_name           False      False          False              False   \n",
      "\n",
      "                   dishes_name  \n",
      "order_id                 False  \n",
      "dishes_id                False  \n",
      "logicprn_name            False  \n",
      "parent_class_name        False  \n",
      "dishes_name               True  \n"
     ]
    }
   ],
   "source": [
    "# 求取特征是否完全相同的矩阵的函数\n",
    "def FeatureEquals(df):\n",
    "    dfEquals=pd.DataFrame([],columns=df.columns,index=df.columns)\n",
    "    for i in df.columns:\n",
    "        for j in df.columns:\n",
    "            dfEquals.loc[i,j]=df.loc[:,i].equals(df.loc[:,j])\n",
    "    return dfEquals\n",
    "detEquals=FeatureEquals(detail1)\n",
    "print(detEquals.iloc[:5,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要删除的列： ['parent_class_name', 'cost', 'discount_amt', 'discount_reason', 'kick_back', 'add_info', 'bar_code', 'add_inprice']\n",
      "删除多余列后detail的特征数目为 10\n"
     ]
    }
   ],
   "source": [
    "# 遍历所有数据\n",
    "lenDet=detEquals.shape[0]\n",
    "dupCol=[]\n",
    "for k in range(lenDet):\n",
    "    for l in range(k+1,lenDet):\n",
    "        if detEquals.iloc[k,l] & \\\n",
    "        (detEquals.columns[l] not in dupCol):\n",
    "            dupCol.append(detEquals.columns[l])\n",
    "print('需要删除的列：',dupCol)\n",
    "detail1.drop(dupCol,axis=1,inplace=True)\n",
    "print('删除多余列后detail的特征数目为',detail1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detial每个特征缺失的数目为:\n",
      " order_id                0\n",
      "dishes_id               0\n",
      "logicprn_name       10037\n",
      "dishes_name             0\n",
      "itemis_add              0\n",
      "counts                  0\n",
      "amounts                 0\n",
      "place_order_time        0\n",
      "picture_file            0\n",
      "emp_id                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('detial每个特征缺失的数目为:\\n',detail1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detail每个特征非缺失的数目为:\n",
      " order_id            10037\n",
      "dishes_id           10037\n",
      "logicprn_name           0\n",
      "dishes_name         10037\n",
      "itemis_add          10037\n",
      "counts              10037\n",
      "amounts             10037\n",
      "place_order_time    10037\n",
      "picture_file        10037\n",
      "emp_id              10037\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('detail每个特征非缺失的数目为:\\n',detail1.notnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 删除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除缺失的列前detial.shap= (10037, 10)\n"
     ]
    }
   ],
   "source": [
    "print('去除缺失的列前detial.shape=',detail1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after, detail.shape= (10037, 9)\n"
     ]
    }
   ],
   "source": [
    "print('after, detail.shape=',\n",
    "     detail1.dropna(axis=1,how='any').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 替换法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detail每个特征的缺失数目为:\n",
      " order_id            0\n",
      "dishes_id           0\n",
      "logicprn_name       0\n",
      "dishes_name         0\n",
      "itemis_add          0\n",
      "counts              0\n",
      "amounts             0\n",
      "place_order_time    0\n",
      "picture_file        0\n",
      "emp_id              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "detail1 = detail1.fillna(-99)\n",
    "print(\"detail每个特征的缺失数目为:\\n\",\n",
    "     detail1.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 插值法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=6、7,y1= [ 76. 102.]\n",
      "x=6、7,y2= [13. 15.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d # 1\n",
    "x=np.array([1,2,3,4,5,8,9,10])\n",
    "y1=np.array([2,8,18,32,50,128,162,200])\n",
    "y2=np.array([3,5,7,9,11,17,19,21])\n",
    "LinearInsValue1=interp1d(x,y1,kind='linear')\n",
    "LinearInsValue2=interp1d(x,y2,kind='linear')\n",
    "print('x=6、7,y1=',LinearInsValue1([6,7]))\n",
    "print('x=6、7,y2=',LinearInsValue2([6,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72. 98.]\n",
      "[13. 15.]\n"
     ]
    }
   ],
   "source": [
    "# 拉格朗日\n",
    "from scipy.interpolate import lagrange\n",
    "LargeInsValue1=lagrange(x,y1)\n",
    "LargeInsValue2=lagrange(x,y2)\n",
    "print(LargeInsValue1([6,7]))\n",
    "print(LargeInsValue2([6,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72. 98.]\n",
      "[13. 15.]\n"
     ]
    }
   ],
   "source": [
    "# 样条插值 2\n",
    "from scipy.interpolate import make_interp_spline\n",
    "xnew=np.array([6,7])\n",
    "SplineInsValue1=make_interp_spline(x,y1)(xnew)\n",
    "SplineInsValue2=make_interp_spline(x,y2)(xnew)\n",
    "print(SplineInsValue1)\n",
    "print(SplineInsValue2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "异常值个数为 209\n",
      "异常值最大值： 10\n",
      "异常值最小值： 3\n"
     ]
    }
   ],
   "source": [
    "# 识别异常值函数\n",
    "def outRange(Ser1):\n",
    "    boolInd = (Ser1.mean()-3*Ser1.std()>Ser1) | \\\n",
    "    (Ser1.mean()+3*Ser1.var()<Ser1)\n",
    "    index = np.arange(Ser1.shape[0])[boolInd]\n",
    "    outrange=Ser1.iloc[index]\n",
    "    return outrange\n",
    "outlier = outRange(detail1['counts'])\n",
    "print(\"异常值个数为\",outlier.shape[0])\n",
    "print(\"异常值最大值：\",outlier.max())\n",
    "print('异常值最小值：',outlier.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAHSCAYAAAAjcvULAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPr0lEQVR4nO3cT6il913H8e/XjqKtFjuTUZJ2JqMgFTKQKHdRMyDSySLEYF2kUKFaJZCFidY/ILqQ6EJwIWIh2QStLVgiMhGUJEhLtBQzNnAnbXCmIwhiZmKiuZ1ZxIVQxJ+L3BmPmTN35nP+zsl9veBw7/Occ57nu7r3fX/Pc0+PMQoAgJv3beseAABg0wgoAICQgAIACAkoAICQgAIACAkoAIDQgVWe7LbbbhvHjh1b5SkBAGZy5syZb44xDk97bqUBdezYsdre3l7lKQEAZtLdr17vOZfwAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAIHTDgOruz3b3m919dmLfwe7+Unf/8+7XDyx3TACAW8fNrEB9rqruf8e+36yqF8YYP1RVL+xuA6zU0aNHq7uvPo4ePbrukYB94oYBNcb4SlVdfsfuj1XV53e//3xV/fSC5wLY09GjR+vixYt177331uuvv1733ntvXbx4UUQBKzHrPVDfP8Z4o6pq9+v3LW4kgBu7Ek8vvvhi3X777fXiiy9ejSiAZVv6TeTd/Uh3b3f39s7OzrJPB+wjp06d2nMbYFlmDaj/6O7bq6p2v755vReOMZ4aY2yNMbYOHz484+kArvXQQw/tuQ2wLLMG1F9X1ad2v/9UVf3VYsYBuDlHjhyp06dP14kTJ+qNN96oEydO1OnTp+vIkSPrHg3YBw7c6AXd/XRV/URV3dbdr1XV41X1+1X1F939cFVdqKqPL3NIgHe6cOFCHT16tE6fPl133HFHVb0dVRcuXFjzZMB+cMOAGmP8zHWeOrngWQAiYglYF59EDgAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQElAAACEBBQAQmiuguvtXu/tcd5/t7qe7+zsXNRgAwK1q5oDq7g9W1S9X1dYY43hVvaeqPrGowQBupLuveQCswryX8A5U1Xd194Gqem9VvT7/SAA3NhlLp06dmrofYFkOzPrGMca/dfcfVNWFqvqvqvriGOOLC5sM4CaMMa5+FU/AqsxzCe8DVfWxqvqBqrqjqt7X3Z+c8rpHunu7u7d3dnZmnxTgHSZXnqZtAyxLX/nrLX5j98er6v4xxsO72z9XVR8ZY/zi9d6ztbU1tre3ZzofwKQrq02TP8Om7QOYVXefGWNsTXtunnugLlTVR7r7vf32T62TVXV+juMBxLq7nnnmGZfvgJWaOaDGGC9V1amqermq/nH3WE8taC6APU2uMj300ENT9wMsy8w3kVdVjTEer6rHFzQLQEQsAevik8gBAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCgAgJKAAAEICCthYhw4dqu6++jh06NC6RwL2CQEFbKRDhw7V5cuX66677qpXX3217rrrrrp8+bKIAlbiwLoHAJjFlXg6e/ZsVVWdPXu2jh8/XufOnVvzZMB+YAUK2FjPP//8ntsAyyKggI31wAMP7LkNsCwCCthIBw8erHPnztXx48frwoULVy/fHTx4cN2jAfuAe6CAjXTp0qU6dOhQnTt3ru68886qejuqLl26tObJgP1AQAEbSywB6+ISHgBASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAIQEFABASEABAITmCqju/t7uPtXd/9Td57v7xxY1GMCNdPc1D4BVmHcF6jNV9TdjjB+uqrur6vz8IwHc2GQsPfroo1P3AyzLgVnf2N3vr6ofr6qfr6oaY3yrqr61mLEAbs4Yo6qqnnjiCfEErMw8K1A/WFU7VfWn3f217v7j7n7fO1/U3Y9093Z3b+/s7MxxOoD/b3Llado2wLL0lb/e4jd2b1XVV6vqxBjjpe7+TFW9Ncb47eu9Z2tra2xvb882KcCEK6tNkz/Dpu0DmFV3nxljbE17bp4VqNeq6rUxxku726eq6kfnOB5ArLvrsccec/kOWKmZA2qM8e9VdbG7P7y762RVfWMhUwHcwOQq05NPPjl1P8CyzHwT+a5fqqovdPd3VNW/VNUvzD8SwM0RS8C6zBVQY4yvV9XUa4MAAO9WPokcACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACB0YN0DAMyqu6/ZN8ZYwyTAfmMFCthIk/H07LPPTt0PsCxWoICNdmXFaYwhnoCVsQIFbKzJladp2wDL0qu8X2Bra2tsb2+v7HzAu9eV1abJn2HT9gHMqrvPjDG2pj1nBQrYaN1dzz33nMt3wEoJKGAjTa4yPfjgg1P3AyyLm8iBjSWWgHWxAgUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAAhAQUAEBJQAAChuQOqu9/T3V/r7mcXMRDAzeruax4Aq7CIFahPV9X5BRwH4KZNxtLdd989dT/AshyY583d/aGq+smq+r2q+rWFTAQQGGNc/V48Aasy7wrUH1XVb1TV/1zvBd39SHdvd/f2zs7OnKcD+D+TK0/TtgGWZeaA6u4Hq+rNMcaZvV43xnhqjLE1xtg6fPjwrKcDuMYrr7yy5zbAssyzAnWiqn6qu/+1qv68qj7a3X+2kKkAblJ31z333OPyHbBSMwfUGOO3xhgfGmMcq6pPVNXfjjE+ubDJAPYwee/T5MrT5H6AZZnrJnKAdRJLwLosJKDGGF+uqi8v4lgAALc6n0QOABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABASUAAAIQEFABA6sO4BAGbV3dfsG2OsYRJgv7ECBWykyXg6efLk1P0Ay2IFCthokytO4glYFStQwMaaXHmatg2wLAIK2FgvvPDCntsAyyKggI3W3XXfffe5fAeslIACNtLkvU+TK0/+Cw9YBTeRAxtLLAHrYgUKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACAkoAAAQgIKACA0c0B195Hu/rvuPt/d57r704scDADgVnVgjvf+d1X9+hjj5e7+nqo6091fGmN8Y0GzAeypu6/ZN8ZYwyTAfjPzCtQY440xxsu73/9nVZ2vqg8uajCAvUyLp732AyzSPCtQV3X3sar6kap6aRHHA7hZkytO4glYlblvIu/u766qZ6rqV8YYb015/pHu3u7u7Z2dnXlPBwCwdnMFVHd/e70dT18YY/zltNeMMZ4aY2yNMbYOHz48z+kAAG4JM1/C67fXyv+kqs6PMf5wcSMB3DyX7YB1mGcF6kRV/WxVfbS7v777eGBBcwHs6Xr/bee/8IBVmHkFaozx91XlTz9gbcQSsC4+iRwAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAICSgAABCAgoAIHRg3QMAt67uXvgxx+PvX/gxl6V/962FH3OMsfBjAqsnoIDr2u+/7MfvrHsC4FblEh4AQEhAAQCEBBQAQEhAAQCEBBQAQEhAAQCEBBQAQEhAAQCEBBQAQEhAAQCEBBQAQEhAAQCEBBQAQEhAAQCEBBQAQEhAAQCEBBQAQEhAAQCEeoyxupN171TVqys7IbBf3FZV31z3EMC7zp1jjMPTnlhpQAEsQ3dvjzG21j0HsH+4hAcAEBJQAAAhAQW8Gzy17gGA/cU9UAAAIStQAAAhAQVsrO7+bHe/2d1n1z0LsL8IKGCTfa6q7l/3EMD+I6CAjTXG+EpVXV73HMD+I6AAAEICCgAgJKAAAEICCgAgJKCAjdXdT1fVP1TVh7v7te5+eN0zAfuDTyIHAAhZgQIACAkoAICQgAIACAkoAICQgAIACAkoAICQgAIACAkoAIDQ/wLjGF4FuziU2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516\n",
      "10\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 根据箱线图识别异常值\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,8))\n",
    "p = plt.boxplot(detail1['counts'].values,notch=True)\n",
    "outlier1=p['fliers'][0].get_ydata()\n",
    "plt.savefig('./异常值识别')\n",
    "plt.show()\n",
    "print(len(outlier1))\n",
    "print(max(outlier1))\n",
    "print(min(outlier1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重操作前订单详情表： (10037, 18)\n",
      "(10037, 10)\n"
     ]
    }
   ],
   "source": [
    "# 去重\n",
    "import pandas as pd\n",
    "detail = pd.read_csv('./data2/detail.csv',index_col=0,encoding='gbk')\n",
    "print('去重操作前订单详情表：',detail.shape)\n",
    "#\n",
    "detail.drop_duplicates(inplace=True)\n",
    "def FeatureEquals(df):\n",
    "    dfEquals=pd.DataFrame([],columns=df.columns,index=df.columns)\n",
    "    for i in df.columns:\n",
    "        for j in df.columns:\n",
    "            dfEquals.loc[i,j]=df.loc[:,i].equals(df.loc[:,j])\n",
    "    return dfEquals\n",
    "detEquals=FeatureEquals(detail)\n",
    "lenDet=detEquals.shape[0]\n",
    "dupCol=[]\n",
    "for k in range(lenDet):\n",
    "    for l in range(k+1,lenDet):\n",
    "        if detEquals.iloc[k,l] & \\\n",
    "        (detEquals.columns[l] not in dupCol):\n",
    "            dupCol.append(detEquals.columns[l])\n",
    "detail.drop(dupCol,axis=1,inplace=True)\n",
    "print(\"去重后：\",detail.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个特征的缺失率为： order_id              0.0%\n",
      "dishes_id             0.0%\n",
      "logicprn_name       100.0%\n",
      "dishes_name           0.0%\n",
      "itemis_add            0.0%\n",
      "counts                0.0%\n",
      "amounts               0.0%\n",
      "place_order_time      0.0%\n",
      "picture_file          0.0%\n",
      "emp_id                0.0%\n",
      "dtype: object\n",
      "处理以后的哥哥特征值确实数目： order_id            0\n",
      "dishes_id           0\n",
      "dishes_name         0\n",
      "itemis_add          0\n",
      "counts              0\n",
      "amounts             0\n",
      "place_order_time    0\n",
      "picture_file        0\n",
      "emp_id              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 统计各个特征的缺失率\n",
    "naRate=(detail.isnull().sum()/\\\n",
    "       detail.shape[0]*100).astype('str')+'%'\n",
    "print('每个特征的缺失率为：',naRate)\n",
    "# 删除全部数据均为缺失的列\n",
    "detail.dropna(axis=1,how=\"all\",inplace=True)\n",
    "print('处理以后的哥哥特征值确实数目：',detail.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max= 1.0\n",
      "min= 1.0\n",
      "售价min= 10.0\n",
      "售价max= 99.0\n"
     ]
    }
   ],
   "source": [
    "# 定义异常值识别与处理函数\n",
    "def outRange(Ser1):\n",
    "    QL=Ser1.quantile(0.25)\n",
    "    QU=Ser1.quantile(0.75)\n",
    "    IQR=QU-QL\n",
    "    Ser1.loc[Ser1>(QU+1.5*IQR)]=QU\n",
    "    Ser1.loc[Ser1<(QU-1.5*IQR)]=QL\n",
    "    return Ser1\n",
    "detail['counts']==outRange(detail['counts'])\n",
    "detail['amounts']=outRange(detail['amounts'])\n",
    "#查看\n",
    "print('max=',detail['counts'].max())\n",
    "print('min=',detail['counts'].min())\n",
    "print('售价min=',detail['amounts'].min())\n",
    "print('售价max=',detail['amounts'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "之前            counts  amounts\n",
      "detail_id                 \n",
      "2956            1       49\n",
      "2958            1       48\n",
      "2961            1       30\n",
      "2966            1       25\n",
      "2968            1       13\n",
      "之后            counts   amounts\n",
      "detail_id                  \n",
      "2956          0.0  0.271186\n",
      "2958          0.0  0.265537\n",
      "2961          0.0  0.163842\n",
      "2966          0.0  0.135593\n",
      "2968          0.0  0.067797\n"
     ]
    }
   ],
   "source": [
    "# 离差标准化\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "detail=pd.read_csv('./data2/detail.csv',index_col=0,\n",
    "                  encoding='gbk')\n",
    "def MinMaxScale(data):\n",
    "    data=(data-data.min())/(data.max()-data.min())\n",
    "    return data\n",
    "# 对售价和销售离差标准化\n",
    "data1=MinMaxScale(detail['counts'])\n",
    "data2=MinMaxScale(detail['amounts'])\n",
    "data3=pd.concat([data1,data2],axis=1)\n",
    "print('之前',detail[['counts','amounts']].head())\n",
    "print('之后',data3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准化之前            counts  amounts\n",
      "detail_id                 \n",
      "2956            1       49\n",
      "2958            1       48\n",
      "2961            1       30\n",
      "2966            1       25\n",
      "2968            1       13\n",
      "标准化之后              counts   amounts\n",
      "detail_id                    \n",
      "2956      -0.177571  0.116671\n",
      "2958      -0.177571  0.088751\n",
      "2961      -0.177571 -0.413826\n",
      "2966      -0.177571 -0.553431\n",
      "2968      -0.177571 -0.888482\n"
     ]
    }
   ],
   "source": [
    "# 自定义标准差标准化函数\n",
    "def StandardScaler(data):\n",
    "    data=(data-data.mean())/data.std()\n",
    "    return data\n",
    "data4=StandardScaler(detail['counts'])\n",
    "data5=StandardScaler(detail['amounts'])\n",
    "data6=pd.concat([data4,data5],axis=1)\n",
    "print('标准化之前',detail[['counts','amounts']].head())\n",
    "print(\"标准化之后\",data6.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小鼠定标准华函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "之前            counts  amounts\n",
      "detail_id                 \n",
      "2956            1       49\n",
      "2958            1       48\n",
      "2961            1       30\n",
      "2966            1       25\n",
      "2968            1       13\n",
      "之后            counts  amounts\n",
      "detail_id                 \n",
      "2956          0.1    0.049\n",
      "2958          0.1    0.048\n",
      "2961          0.1    0.030\n",
      "2966          0.1    0.025\n",
      "2968          0.1    0.013\n"
     ]
    }
   ],
   "source": [
    "def DecimalScaler(data):\n",
    "    data=data/10**np.ceil(np.log10(data.abs().max()))\n",
    "    return data\n",
    "data7=DecimalScaler(detail['counts'])\n",
    "data8=DecimalScaler(detail['amounts'])\n",
    "data9=pd.concat([data7,data8],axis=1)\n",
    "print('之前',detail[['counts','amounts']].head())\n",
    "print('之后',data9.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:            counts  amounts\n",
      "detail_id                 \n",
      "2956            1       49\n",
      "2958            1       48\n",
      "2961            1       30\n",
      "2966            1       25\n",
      "2968            1       13\n",
      "after:              counts   amounts\n",
      "detail_id                    \n",
      "2956      -0.177571  0.116671\n",
      "2958      -0.177571  0.088751\n",
      "2961      -0.177571 -0.413826\n",
      "2966      -0.177571 -0.553431\n",
      "2968      -0.177571 -0.888482\n"
     ]
    }
   ],
   "source": [
    "# 自定义标准差标准化函数\n",
    "def StandardScaler(data):\n",
    "    data=(data-data.mean())/data.std()\n",
    "    return data\n",
    "data4=StandardScaler(detail['counts'])\n",
    "data5=StandardScaler(detail['amounts'])\n",
    "data6=pd.concat([data4,data5],axis=1)\n",
    "print('before:',detail[['counts','amounts']].head())\n",
    "print('after:',data6.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哑变量处理前数据： 0     蒜蓉生蚝\n",
      "1    蒙古烤羊腿\n",
      "2     大蒜苋菜\n",
      "3    芝麻烤紫菜\n",
      "4      蒜香包\n",
      "5      白斩鸡\n",
      "Name: dishes_name, dtype: object\n",
      "哑变量处理后数据：    大蒜苋菜  白斩鸡  芝麻烤紫菜  蒙古烤羊腿  蒜蓉生蚝  蒜香包\n",
      "0     0    0      0      0     1    0\n",
      "1     0    0      0      1     0    0\n",
      "2     1    0      0      0     0    0\n",
      "3     0    0      1      0     0    0\n",
      "4     0    0      0      0     0    1\n",
      "5     0    1      0      0     0    0\n"
     ]
    }
   ],
   "source": [
    "# 哑变量处理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "detail=pd.read_csv('./data2/detail.csv',encoding='gbk')\n",
    "data=detail.loc[0:5,'dishes_name']\n",
    "print('哑变量处理前数据：',data)\n",
    "print('哑变量处理后数据：',pd.get_dummies(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "离散后5条记录售价\n",
      " (0.823, 36.4]     5461\n",
      "(36.4, 71.8]      3157\n",
      "(71.8, 107.2]      839\n",
      "(142.6, 178.0]     426\n",
      "(107.2, 142.6]     154\n",
      "Name: amounts, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 等宽离散\n",
    "price=pd.cut(detail['amounts'],5)\n",
    "print('离散后5条记录售价\\n',price.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "离散化后的类别分布状况 \n",
      " (18.0, 32.0]     2107\n",
      "(39.0, 58.0]     2080\n",
      "(32.0, 39.0]     1910\n",
      "(1.0, 18.0]      1891\n",
      "(58.0, 178.0]    1863\n",
      "Name: amounts, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 等频法\n",
    "def SameRateCut(data,k):\n",
    "    w=data.quantile(np.arange(0,1+1.0/k,1.0/k))\n",
    "    data=pd.cut(data,w)\n",
    "    return data\n",
    "result=SameRateCut(detail['amounts'],5).value_counts()\n",
    "print('离散化后的类别分布状况','\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聚类离散化以后的类别分布 \n",
      " (22.31, 43.51]       3690\n",
      "(43.51, 73.945]      2474\n",
      "(0.0, 22.31]         2454\n",
      "(73.945, 131.858]     993\n",
      "(131.858, 178.0]      426\n",
      "Name: amounts, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 聚类分析离散化 3\n",
    "def KmeanCut(data,k):\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmodel=KMeans(n_clusters=k,n_jobs=4)\n",
    "    kmodel.fit(data.values.reshape((len(data),1)))\n",
    "    \n",
    "    c=pd.DataFrame(kmodel.cluster_centers_).sort_values(0)\n",
    "    w=c.rolling(2).mean().iloc[1:]\n",
    "    w=[0]+list(w[0])+[data.max()]\n",
    "    data=pd.cut(data,w)\n",
    "    return data\n",
    "result=KmeanCut(detail['amounts'],5).value_counts()\n",
    "print('聚类离散化以后的类别分布','\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
